---
title: "2018_05_25"
author: "John D."
date: "May 23, 2018"
output:
  html_document:
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 10 Unsupervised Learning

## 10.4 Lab 1: Principal Components Analysis

```{r}
states <- row.names(USArrests)
states

names(USArrests)
apply(USArrests, 2, mean)
apply(USArrests, 2, var)

pr.out <- prcomp(USArrests, scale = T)
names(pr.out)

pr.out$center
pr.out$scale
pr.out$rotation

dim(pr.out$x)
head(pr.out$x)

biplot(pr.out, scale = 0, cex = .5)

pr.out$rotation <- -pr.out$rotation
pr.out$x <- -pr.out$x
biplot(pr.out, scale = 0, cex = .5)

pr.out$sdev
pr.var <- pr.out$sdev^2
pr.var

pve <- pr.var/sum(pr.var)
pve

plot(pve, xlab = "Principal Component", ylab = "Proportion of Variance Explained", ylim = c(0,1), type = "b")
plot(cumsum(pve), xlab = "Principal Component", ylab = "Cumulative Proportion of Variance Explained", ylim = c(0,1), type = "b")

a <- c(1,2,8,-3)
cumsum(a)
```

## 10.5 Lab 2: Clustering
### 10.5.1 K-Means Clustering

```{r}
set.seed(2)
x <- matrix(rnorm(50*2), ncol = 2)
x[1:25,1] <- x[1:25,1] + 3
x[1:25,2] <- x[1:25,2] - 4

km.out <- kmeans(x, 2, nstart = 20)
km.out$cluster

plot(x, col = (km.out$cluster + 1), main = "K-Means Clustering Results with K=2", xlab = "", ylab = "", pch = 20, cex = 2)

set.seed(4)
km.out <- kmeans(x, 3, nstart = 20)
km.out

set.seed(3)
km.out <- kmeans(x, 3, nstart = 1)
km.out$tot.withinss
km.out <- kmeans(x, 3, nstart = 20)
km.out$tot.withinss
```

### 10.5.2 Hierarchical Clustering

```{r}
hc.complete <- hclust(dist(x), method = "complete")
hc.average <- hclust(dist(x), method = "average")
hc.single <- hclust(dist(x), method = "single")

par(mfrow = c(1,1))
plot(hc.complete, main = "Complete Linkage ", xlab = "", sub = "", cex = .9)
plot(hc.average, main = "Average Linkage ", xlab = "", sub = "", cex = .9)
plot(hc.single, main = "Single Linkage ", xlab = "", sub = "", cex = .9)

cutree(hc.complete, 2)
cutree(hc.average, 2)
cutree(hc.single, 2)

cutree(hc.single, 4)

xsc <- scale(x)
par(mfrow = c(1,1))
plot(hclust(dist(xsc), method = "complete"), main = "Hierarchical Clustering with Scaled Features")

x <- matrix(rnorm(30*3), ncol = 3)
dd <- as.dist(1 - cor(t(x)))
plot(hclust(dd, method ="complete"), main = "Complete Linkage with Correlation-Based Distance", xlab = "", sub = "")
```

## 10.6 Lab 3: NCI60 Data Example

```{r}
library(ISLR)
nci.labs <- NCI60$labs
nci.data <- NCI60$data
dim(nci.data)
nci.labs[1:4]
table(nci.labs)
```

### 10.6.1 PCA on the NCI60 Data

```{r}
pr.out <- prcomp(nci.data, scale = TRUE)

Cols <- function(vec){
  cols <- rainbow(length(unique(vec)))
  return(cols[as.numeric(as.factor(vec))])
}

par(mfrow = c(1,2))
plot(pr.out$x[,1:2], col = Cols(nci.labs), pch = 19, xlab = "Z1", ylab = "Z2")
plot(pr.out$x[,c(1,3)], col = Cols(nci.labs), pch = 19, xlab = "Z1", ylab = "Z3")

summary(pr.out)
par(mfrow = c(1,1))
plot(pr.out)

pve  <- 100 * pr.out$sdev^2 / sum(pr.out$sdev^2)
par(mfrow = c(1,2))
plot(pve, type = "o", ylab = "PVE", xlab = "Principal Component", col = "blue")
plot(cumsum(pve), type = "o", ylab = "Cumulative PVE", xlab = "Principal Component", col = "brown3")

```

### 10.6.2 Clustering the Observations of the NCI60 Data

```{r}
sd.data=scale(nci.data)

par(mfrow = c(1,1))
data.dist <- dist(sd.data)
plot(hclust(data.dist), labels = nci.labs, main = "Complete Linkage ", xlab = "", sub = "", ylab = "")
plot(hclust(data.dist, method = "average"), labels = nci.labs, main = "Average Linkage", xlab = "", sub = "", ylab = "")
plot(hclust(data.dist, method = "single"), labels = nci.labs, main = "Single Linkage", xlab = "", sub = "", ylab = "")

hc.out <- hclust(dist(sd.data))
hc.clusters  <- cutree(hc.out, 4)
table(hc.clusters, nci.labs)

plot(hc.out, labels = nci.labs)
abline(h=139, col="red")

hc.out

set.seed(2)
km.out <- kmeans(sd.data, 4, nstart = 20)
km.clusters  <- km.out$cluster
table(km.clusters, hc.clusters)

hc.out <- hclust(dist(pr.out$x[,1:5]))
plot(hc.out, labels = nci.labs, main = "Hier. Clust. on First Five Score Vectors")
table(cutree(hc.out ,4), nci.labs)
```

## 10.7 Exercises
### Conceptual

#### 2. Suppose that we have four observations, for which we compute a dissimilarity matrix, given by
```{r}
matrix(c(0,0.3,0.4,0.7,0.3,0,0.5,0.8,0.4,0.5,0,0.45,0.7,0.8,0.45,0), ncol = 4, nrow = 4)
```
#### For instance, the dissimilarity between the first and second observationsis 0.3, and the dissimilarity between the second and fourth observations is 0.8.

  (a) On the basis of this dissimilarity matrix, sketch the dendrogram that results from hierarchically clustering these four observations using complete linkage. Be sure to indicate on the plot the height at which each fusion occurs, as well as the observations corresponding to each leaf in the dendrogram.  

  (b) Repeat (a), this time using single linkage clustering  

[dendrograms]()

  (c) Suppose that we cut the dendogram obtained in (a) such that two clusters result. Which observations are in each cluster?  
  1 and 2 will be in one cluster. 3 and 4 will be in the other cluster

  (d) Suppose that we cut the dendogram obtained in (b) such that two clusters result. Which observations are in each cluster?  
  1, 2, and 3 will be in one cluster and 4 will be in the other
  
  (e) It is mentioned in the chapter that at each fusion in the dendrogram, the position of the two clusters being fused can be swapped without changing the meaning of the dendrogram. Draw a dendrogram that is equivalent to the dendrogram in (a), for which two or more of the leaves are repositioned, but for which the meaning of the dendrogram is the same.  
  
#### 3. In this problem, you will perform K-means clustering manually, with K = 2, on a small example with n = 6 observations and p = 2 features. The observations are as follows

```{r}
dat <- data.frame(X1 = c(1,1,0,5,6,4), X2 = c(4,3,4,1,2,0))
```

  (a) Plot the observations  
```{r}
plot(dat)
```

  (b) Randomly assign a cluster label to each observation. You can use the sample() command in R to do this. Report the cluster labels for each observation.  

```{r}
set.seed(324)
groupings <- sample(c(1,2),6, replace = T)
plot(dat, col = groupings + 2)
```

  (c) Compute the centroid for each cluster

```{r}
centroid1 <- c(mean(dat[groupings == 1, 1]), mean(dat[groupings == 1, 2]))
centroid2 <- c(mean(dat[groupings == 2, 1]), mean(dat[groupings == 2, 2]))
plot(dat, col=(groupings + 2))
points(centroid1[1], centroid1[2], col = 3, pch = 4)
points(centroid2[1], centroid2[2], col = 4, pch = 4)
```

  (d) Assign each observation to the centroid to which it is closest, in terms of Euclidean distance. Report the cluster labels for each observation
  
```{r}
groupings <- c(2,2,2,1,1,1)
plot(dat, col=(groupings + 2))
points(centroid1[1], centroid1[2], col = 3, pch = 4)
points(centroid2[1], centroid2[2], col = 4, pch = 4)
```

  (e) Repeat (c) and (d) until the answers obtained stop changing.  
  Done
  
  (f) In your plot from (a), color the observations according to the cluster labels obtained.  

```{r}
plot(dat, col=(groupings + 2))
```

#### 4. Suppose that for a particular data set, we perform hierarchical clustering using single linkage and using complete linkage. We obtain two dendrograms.

  (a) At a certain point on the single linkage dendrogram, the clusters {1, 2, 3} and {4, 5} fuse. On the complete linkage dendrogram, the clusters {1, 2, 3} and {4, 5} also fuse at a certain point. Which fusion will occur higher on the tree, or will they fuse at the same height, or is there not enough information to tell?  
  There is not enough information to tell. The largest and smallest distance between observations in a cluster could be the same.
  
  (b) At a certain point on the single linkage dendrogram, the clusters {5} and {6} fuse. On the complete linkage dendrogram, the clusters {5} and {6} also fuse at a certain point. Which fusion will occur higher on the tree, or will they fuse at the same height, or is there not enough information to tell?  
  They will fuse at the same height. Complete fuses clusters when the largest distance between observations between 2 clusters is the overall smallest distance. Single will fuse clusters based on the smallest distance. If there is only one observation in each cluster then the largest and smallest distance is the same and complete and single linkage operate the same

### Applied
#### 9. Consider the USArrests data. We will now perform hierarchical clustering on the states.
  (a) Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states.  
```{r}
set.seed(123)
hc.complete <- hclust(dist(USArrests), method = "complete")
plot(hc.complete)
```

  (b) Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters? 
```{r}
cut.hc.complete <- cutree(hc.complete, 3)
cut.hc.complete
table(cut.hc.complete)
```

  (c) Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation one.  
```{r}
shc.complete <- hclust(dist(scale(USArrests)), method = "complete")
plot(shc.complete)
cut.shc.complete <- cutree(shc.complete, 3)
cut.shc.complete
table(cut.shc.complete)
```

  (d) What effect does scaling the variables have on the hierarchical clustering obtained? In your opinion, should the variables be scaled before the inter-observation dissimilarities are computed? Provide a justification for your answer.  
```{r}
summary(USArrests)
table(cut.hc.complete, cut.shc.complete)
```
Differences appear after you scale the data. Scaling should be done in this case because the data is measured in different units

#### 10. In this problem, you will generate simulated data, and then perform PCA and K-means clustering on the data.

  (a) Generate a simulated data set with 20 observations in each of three classes (i.e. 60 observations total), and 50 variables.  
```{r}
set.seed(321)
dat <- matrix(rnorm(20 * 3 * 50, mean = 0), ncol = 50)
dat[1:20,1] <- dat[1:20,1] + 3 # shift means
dat[21:40,2] <- dat[21:40,2] - 3 # shift means
dat[41:60,3] <- dat[41:60,3] + 4 # shift means
groupings <- c(rep(1, 20), rep(2, 20), rep(3, 20))
```

  (b) Perform PCA on the 60 observations and plot the first two principal component score vectors. Use a different color to indicate the observations in each of the three classes. If the three classes appear separated in this plot, then continue on to part (c). If not, then return to part (a) and modify the simulation so that there is greater separation between the three classes. Do not continue to part (c) until the three classes show at least some separation in the first two principal component score vectors.
  
```{r}
pr.out <- prcomp(dat)
plot(pr.out$x[, 1:2], col = groupings, xlab = "Principal Component 1 Score", ylab = "Principal Component 2 Score", pch = 19)
```

  (c) Perform K-means clustering of the observations with K = 3. How well do the clusters that you obtained in K-means clustering compare to the true class labels? *Hint: You can use the table() function in R to compare the true class labels to the class labels obtained by clustering. Be careful how you interpret the results: K-means clustering will arbitrarily number the clusters, so you cannot simply check whether the true class labels and clustering labels are the same.*  
```{r}
km.out <- kmeans(dat, 3, nstart = 50)
km.out$cluster
table(groupings,km.out$cluster)
```
One observation was placed in the wrong group

  (d) Perform K-means clustering with K = 2. Describe your results.  
```{r}
km.out <- kmeans(dat, 2, nstart = 50)
km.out$cluster
table(groupings,km.out$cluster)
```
Looks like it condensed two separate clusters into one cluster while keeping the other relatively intact

  (e) Now perform K-means clustering with K = 4, and describe your results.  
```{r}
km.out <- kmeans(dat, 4, nstart = 50)
km.out$cluster
table(groupings,km.out$cluster)
```
Looks like it broke one cluster into two separate clusters while keeping the other 2 relatively intact

  (f) Now perform K-means clustering with K = 3 on the first two principal component score vectors, rather than on the raw data. That is, perform K-means clustering on the 60 × 2 matrix of which the first column is the first principal component score vector, and the second column is the second principal component score vector. Comment on the results.
```{r}
km.out <- kmeans(pr.out$x[, 1:2], 3, nstart = 50)
km.out$cluster
table(groupings,km.out$cluster)
```
Same result as in (c). Still get one observation grouped incorrectly.

  (g) Using the scale() function, perform K-means clustering with K = 3 on the data after scaling each variable to have standard deviation one. How do these results compare to those obtained in (b)? Explain.  
```{r}
km.out <- kmeans(scale(dat), 3, nstart = 50)
km.out$cluster
table(groupings,km.out$cluster)
```
Made things a lot worse. In this case scaling was detrimental to the ability to cluster.

#### 11. On the book website, www.StatLearning.com, there is a gene expression data set (Ch10Ex11.csv) that consists of 40 tissue samples with measurements on 1,000 genes. The first 20 samples are from healthy patients, while the second 20 are from a diseased group.

  (a) Load in the data using read.csv(). You will need to select header = F.  
```{r}
dat <- read.csv(url("http://www-bcf.usc.edu/~gareth/ISL/Ch10Ex11.csv"), header = F)
```

  (b) Apply hierarchical clustering to the samples using correlation-based distance, and plot the dendrogram. Do the genes separate the samples into the two groups? Do your results depend on the type of linkage used?  
```{r}
ddat <- as.dist(1 - cor(dat))
plot(hclust(ddat, method ="complete"), main = "Complete Linkage with Correlation-Based Distance", xlab = "", sub = "")
plot(hclust(ddat, method ="average"), main = "Average Linkage with Correlation-Based Distance", xlab = "", sub = "")
plot(hclust(ddat, method ="single"), main = "Single Linkage with Correlation-Based Distance", xlab = "", sub = "")

```
With complete and single the samples separate into two groups. The same cannot be said for average

  (c) Your collaborator wants to know which genes differ the most across the two groups. Suggest a way to answer this question, and apply it here.  
```{r}
pr.out <- prcomp(t(dat))
pr.var <- pr.out$sdev^2
pve <- pr.var/sum(pr.var)
pve
par(mfrow=c(1,2))
plot(pve, xlab = "Principal Component", ylab = "Proportion of Variance Explained", ylim = c(0,1), type = "b")
plot(cumsum(pve), xlab = "Principal Component", ylab = "Cumulative Proportion of Variance Explained", ylim = c(0,1), type = "b")
top <- order(abs(pr.out$rotation[,1]), decreasing = T)
top[1:10]
```
Originally we were looking at our ability to separate samples based on gene expression. Now were want to see what genes were causing driving the clustering. For this we will transpose the data and perform PCA, then look at the variance explained. From their select the loadings from the selected principal components. We then select the 20 genes which corresponding to the 20 most influential genes in the first principal component. Could probably look at more principal components though.
