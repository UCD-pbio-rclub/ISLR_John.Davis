---
title: "2018_02_27"
author: "John D."
date: "February 27, 2018"
output:
  html_document:
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 6.5.3 Choosing Among Models Using the Validation Set Approach and Cross-Validation

```{r}
library(ISLR)
library(leaps)
set.seed(1)
train <- sample(c(TRUE ,FALSE), nrow(Hitters),rep=TRUE)
test <- (!train)
regfit.best <- regsubsets(Salary ~ ., data=Hitters[train ,], nvmax=19)
test.mat <- model.matrix(Salary ~ ., data=Hitters [test ,])
val.errors <- rep(NA ,19)
for(i in 1:19){
  coefi <- coef(regfit.best, id=i)
  pred <- test.mat[,names(coefi)]%*%coefi
  val.errors[i] <- mean((na.omit(Hitters$Salary[test])-pred)^2)
}
val.errors
which.min(val.errors)
coef(regfit.best ,10)

predict.regsubsets <- function(object, newdata, id, ...){
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form, newdata)
  coefi <- coef(object, id=id)
  xvars <- names(coefi)
  mat[,xvars]%*%coefi
}

regfit.best <- regsubsets(Salary ~ .,data=Hitters ,nvmax=19)
coef(regfit.best, 10)


k=10
set.seed(1)
folds <- sample(1:k, nrow(Hitters), replace=TRUE)
cv.errors <- matrix(NA,k,19, dimnames=list(NULL, paste(1:19)))

for(j in 1:k){
   best.fit <- regsubsets(Salary ~ ., data=Hitters[folds!=j,], nvmax=19)
    for(i in 1:19){
       pred <- predict(best.fit, Hitters[folds==j,], id=i)
      cv.errors[j,i] <- mean((na.omit(Hitters$Salary[folds==j])-pred)^2)
    }
}

mean.cv.errors <- apply(cv.errors, 2, mean)
mean.cv.errors
par(mfrow=c(1,1))
plot(mean.cv.errors, type='b')

reg.best <- regsubsets(Salary ~ ., data=Hitters, nvmax=19)
coef(reg.best, 10)
```

# 6.6 Lab 2: Ridge Regression and the Lasso
## 6.6.1 Ridge Regression

```{r}
library(glmnet)
x <- model.matrix(Salary ~ ., Hitters)[,-1]
y <- na.omit(Hitters$Salary)
grid <- 10^seq(10, -2, length =100)
ridge.mod <- glmnet(x, y, alpha=0, lambda=grid)
dim(coef(ridge.mod))
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))

ridge.mod$lambda[60]
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))
predict(ridge.mod, s=50, type="coefficients")[1:20,]

set.seed(1)
train <- sample(1:nrow(x),nrow(x)/2)
test <- (-train)
y.test <- y[test]

ridge.mod <- glmnet(x[train,], y[train], alpha=0, lambda=grid, thresh=1e-12)
ridge.pred <- predict(ridge.mod, s=4, newx=x[test,])
mean((ridge.pred-y.test)^2)
mean((mean(y[train])-y.test)^2)

ridge.pred <- predict(ridge.mod, s=1e10, newx=x[test,])
mean((ridge.pred-y.test)^2)

ridge.pred <- predict(ridge.mod, s=0, newx=x[test,], exact = T, x=x[train,], y=y[train])
mean((ridge.pred-y.test)^2)

lm(y ~ x, subset=train)
predict(ridge.mod, s=0, exact=T, type="coefficients", x=x[train,], y=y[train])[1:20,]

set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha=0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam

ridge.pred <- predict(ridge.mod, s=bestlam, newx=x[test,])
mean((ridge.pred - y.test)^2)

out <- glmnet(x, y, alpha=0)
predict(out, type="coefficients", s=bestlam)[1:20,]
```

# 6.8 Exercises

### 3. Suppose we estimate the regression coefficients in a linear regression model by minimizing (equation) for a particular value of s. For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer.

  (a) As we increase s from 0, the training RSS will:  
    Decrease steadily  
  (b) Repeat (a) for test RSS.  
    Decrease initially, and then eventually start increasing in a U shape  
  (c) Repeat (a) for variance.  
    Steadily increase.  
  (d) Repeat (a) for (squared) bias.  
    Steadily decrease.  
  (e) Repeat (a) for the irreducible error.  
    Remain constant.
    
### 4. Suppose we estimate the regression coefficients in a linear regression model by minimizing (function) for a particular value of λ. For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer.

  (a) As we increase λ from 0, the training RSS will:  
    Steadily increase.  
  (b) Repeat (a) for test RSS.  
    Decrease initially, and then eventually start increasing in a U shape.  
  (c) Repeat (a) for variance.  
    Steadily decrease.  
  (d) Repeat (a) for (squared) bias.  
    Steadily increase.  
  (e) Repeat (a) for the irreducible error.  
    Remain constant.
    
### 5. It is well-known that ridge regression tends to give similar coefficient values to correlated variables, whereas the lasso may give quite different coefficient values to correlated variables. We will now explore this property in a very simple setting. Suppose that n = 2, p = 2, x11 = x12, x21 = x22. Furthermore, suppose that y1 +y2 = 0 and x11 +x21 = 0 and x12 +x22 = 0, so that the estimate for the intercept in a least squares, ridge regression, or lasso model is zero: βˆ0 = 0.

  (a) Write out the ridge regression optimization problem in this setting.  
    Uh...
  (b) Argue that in this setting, the ridge coefficient estimates satisfy βˆ1 = βˆ2.  
    Uh...
    
```{r}
X <- matrix(c(1,-1,1,-1),2,2)
Y <- c(-1,1)
X
Y
ridge.mod <- glmnet(X, Y, alpha=0)
coef(ridge.mod)[,3]
```

### 9. In this exercise, we will predict the number of applications received using the other variables in the College data set.

  (a) Split the data set into a training set and a test set.  
```{r}
summary(College)
set.seed(1)
train <- sample(c(TRUE ,FALSE), nrow(College),rep=TRUE)
test <- (!train)
y.test <- y[test]
```

  (b) Fit a linear model using least squares on the training set, and report the test error obtained.  
```{r}
lm.fit <- lm(Apps ~ ., data = College[train,])
summary(lm.fit)
pred <- predict(lm.fit, College[test,])
test.err <- mean((College[test,]$Apps - pred)^2)
test.err
```

  (c) Fit a ridge regression model on the training set, with λ chosen by cross-validation. Report the test error obtained.  
```{r}
library(dplyr)
X <- College[train,] %>% select(-Apps) %>% mutate(Private = as.numeric(Private)-1) %>% as.matrix()
Y <- College[train,] %>% select(Apps) %>% as.matrix()
X.test <- College[test,] %>% select(-Apps) %>% mutate(Private = as.numeric(Private)-1) %>% as.matrix()
Y.test <- College[test,] %>% select(Apps) %>% as.matrix()
ridge.mod <- glmnet(X, Y, alpha=0)
cv.out <- cv.glmnet(X, Y, alpha=0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam

ridge.pred <- predict(ridge.mod, s=bestlam, newx=X.test)
mean((Y.test-ridge.pred)^2)
```

